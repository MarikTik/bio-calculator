#!/usr/bin/env python3
import sys
from typing import List, Dict
from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from multiprocessing import Pool
import json

def get_driver() -> webdriver.Chrome:
     """Initialize the WebDriver."""
     chrome_options = ChromeOptions()
     chrome_options.add_argument("--headless")
     # chrome_options.add_argument("--no-sandbox")
     # chrome_options.add_argument("--disable-dev-shm-usage")
     # chrome_options.add_argument("--disable-gpu")

     return webdriver.Chrome(options=chrome_options)

def scrape_content(url: str) -> List[str]:
     """Scrape the DNA sequences from the given URL."""
     driver = get_driver()
     driver.get(url)
     estimated_wait = 10
     WebDriverWait(driver, estimated_wait).until(
          EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.ff_line"))
     )

     spans = [span.text for span in driver.find_elements(By.CSS_SELECTOR, "span.ff_line")]
     driver.quit()
     return spans

def scrape_url(description: str, url: str) -> Dict:
     """Wrapper function to scrape a single URL and structure the result."""
     spans = scrape_content(url)
     return {"description": description, "url": url, "genes": [span for span in spans]}

def main():
     with open("venv/intermediate.json") as json_file:
     
          if len(sys.argv) == 1:
               max_processes = 10
          elif len(sys.argv) == 2:
               max_processes = int(sys.argv[1])
          else:
               print("Usage: python download_data [max_processes]")
               sys.exit(1)

          json_sections = json.load(json_file)
          results = dict()
          for section in json_sections:
               links: dict = json_sections[section]

               with Pool(processes=min(len(links), max_processes)) as pool:
                    results[section] = pool.starmap(scrape_url, links.items())

          with open("data/data.json", "w") as data_file:
               json.dump(results, data_file, indent=4)

if __name__ == '__main__':
     main()
